{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dda1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Constants — make sure these match your training settings\n",
    "MAX_HISTORY = 50\n",
    "MAX_TITLE_LEN = 100\n",
    "PAD_ID = 0  # [PAD] token for BERT\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_titles(titles: List[str], max_len: int = MAX_TITLE_LEN) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tokenizes and pads a list of article titles using BERT tokenizer.\n",
    "    Returns: token_ids (N, max_len), padding_mask (N, max_len)\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        titles,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=False  # NRMS does not expect [CLS] or [SEP]\n",
    "    )\n",
    "    token_ids = encodings[\"input_ids\"]       # (N, max_len)\n",
    "    padding_mask = ~encodings[\"attention_mask\"].bool()  # True = pad\n",
    "    return token_ids, padding_mask\n",
    "\n",
    "def recommend_topk_from_titles(\n",
    "    model: torch.nn.Module,\n",
    "    history_titles: List[str],\n",
    "    candidate_titles: List[str],\n",
    "    topk: int = 5,\n",
    "    device: torch.device = torch.device(\"cpu\")\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Recommends top-k titles from a list of candidate article titles,\n",
    "    given a user's clicked history (also as titles).\n",
    "\n",
    "    Args:\n",
    "        model:            Trained NRMS model.\n",
    "        history_titles:   List of clicked article titles (strings).\n",
    "        candidate_titles: List of candidate article titles (strings).\n",
    "        topk:             Number of top articles to return.\n",
    "        device:           Torch device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        List of top-k recommended article titles (strings).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Tokenize history and candidates\n",
    "    hist_tokens, hist_mask = tokenize_titles(history_titles, max_len=MAX_TITLE_LEN)\n",
    "    cand_tokens, cand_mask = tokenize_titles(candidate_titles, max_len=MAX_TITLE_LEN)\n",
    "\n",
    "    # 2. Pad history to MAX_HISTORY size\n",
    "    num_hist = len(history_titles)\n",
    "    if num_hist < MAX_HISTORY:\n",
    "        pad_len = MAX_HISTORY - num_hist\n",
    "        pad_tokens = torch.full((pad_len, MAX_TITLE_LEN), PAD_ID, dtype=torch.long)\n",
    "        pad_mask = torch.ones((pad_len, MAX_TITLE_LEN), dtype=torch.bool)\n",
    "        hist_tokens = torch.cat([pad_tokens, hist_tokens], dim=0)\n",
    "        hist_mask = torch.cat([pad_mask, hist_mask], dim=0)\n",
    "    elif num_hist > MAX_HISTORY:\n",
    "        hist_tokens = hist_tokens[-MAX_HISTORY:]\n",
    "        hist_mask = hist_mask[-MAX_HISTORY:]\n",
    "\n",
    "    # 3. Add batch dimension\n",
    "    clicked_ids = hist_tokens.unsqueeze(0).to(device)    # (1, MAX_HISTORY, MAX_TITLE_LEN)\n",
    "    clicked_mask = hist_mask.unsqueeze(0).to(device)     # (1, MAX_HISTORY, MAX_TITLE_LEN)\n",
    "    cand_ids = cand_tokens.unsqueeze(0).to(device)       # (1, K, MAX_TITLE_LEN)\n",
    "    cand_mask = cand_mask.unsqueeze(0).to(device)        # (1, K, MAX_TITLE_LEN)\n",
    "\n",
    "    # 4. Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = model(clicked_ids, clicked_mask, cand_ids, cand_mask)  # (1, K)\n",
    "\n",
    "    scores = logits.squeeze(0)  # (K,)\n",
    "    topk_vals, topk_idxs = torch.topk(scores, k=min(topk, scores.size(0)))\n",
    "\n",
    "    return [candidate_titles[i] for i in topk_idxs.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c1012",
   "metadata": {},
   "source": [
    "# Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrms import NRMS\n",
    "\n",
    "CHECK_PATH = './checkpoints/checkpoint_epoch1_step1000.pt'\n",
    "\n",
    "# This has to be the same as the trained model\n",
    "model = NRMS(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_embed=512,\n",
    "    n_heads=8,\n",
    "    d_mlp=2048,\n",
    "    news_layers=1,\n",
    "    user_layers=1,\n",
    "    dropout=0.1,\n",
    "    pad_max_len=MAX_TITLE_LEN \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e3f43f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations:\n",
      " • TECH vibes lmao: Xiami's New iPhone 16 Pro Max Features Leaked\n",
      " • NVIDIA Surpasses $3 Trillion Market Cap Amid AI Chip Boom\n",
      " • Broadway Revival of The Phantom of the Opera Sells Out in Record Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danik\\AppData\\Local\\Temp\\ipykernel_10116\\3683566471.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CHECK_PATH, map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(CHECK_PATH, map_location=\"cpu\"))\n",
    "\n",
    "history = [\n",
    "    \"OpenAI Unveils GPT-5 Major Leap in Multimodal AI Capabilities\",\n",
    "    \"Apple Confirms WWDC 2025 Event Expected Focus on Vision Pro 2 and AI Tools\",\n",
    "    \"Google Integrates Gemini AI Across Android 15 What It Means for Users\",\n",
    "]\n",
    "\n",
    "candidates = [\n",
    "    \"UN Urges Immediate Ceasefire in Sudan as Humanitarian Crisis Deepens\",\n",
    "    \"Frog population increases in Amazon\",\n",
    "    \"NVIDIA Surpasses $3 Trillion Market Cap Amid AI Chip Boom\",\n",
    "    \"New 2025 Study Reveals Link Between Sleep Quality and Mental Health\",\n",
    "    \"Global Inflation Slows, But Food Prices Remain Stubbornly High\",\n",
    "    \"Israel and Hamas Agree to Extend Ceasefire for Humanitarian Aid\",\n",
    "    \"TECH vibes lmao: Xiami's New iPhone 16 Pro Max Features Leaked\",\n",
    "    \"Broadway Revival of The Phantom of the Opera Sells Out in Record Time\",\n",
    "]\n",
    "\n",
    "top_titles = recommend_topk_from_titles(\n",
    "    model=model,\n",
    "    history_titles=history,\n",
    "    candidate_titles=candidates,\n",
    "    topk=3,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "print(\"Top recommendations:\")\n",
    "for title in top_titles:\n",
    "    print(\" •\", title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192f040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
