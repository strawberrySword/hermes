{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danik/miniconda3/envs/recSys/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nrms import NRMS\n",
    "from typing import List, Dict\n",
    "from torch.optim.adamw import AdamW\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, get_linear_schedule_with_warmup\n",
    "from data import load_and_tokenize_news, load_behaviors, MindDataset, mind_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = './data/MIND_'\n",
    "\n",
    "\n",
    "MAX_TITLE_LEN = 100   # each headline → exactly MAX_TITLE_LEN tokens (truncated/padded)\n",
    "MAX_HISTORY  = 50     # each user’s clicked history → exactly MAX_HISTORY articles\n",
    "BACTH_SIZE = 6\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "train_news_dict = load_and_tokenize_news(BASE_DATA_DIR+'train/news.tsv', tokenizer, MAX_TITLE_LEN, include_topics=True)\n",
    "train_samples   = load_behaviors(BASE_DATA_DIR+'train/behaviors.tsv', train_news_dict, MAX_HISTORY, include_topics=True)\n",
    "\n",
    "val_news_dict = load_and_tokenize_news(BASE_DATA_DIR+'val/news.tsv', tokenizer, MAX_TITLE_LEN, include_topics=True)\n",
    "val_samples   = load_behaviors(BASE_DATA_DIR+'val/behaviors.tsv', val_news_dict, MAX_HISTORY, include_topics=True)\n",
    "\n",
    "\n",
    "train_dataset = MindDataset(train_samples, include_topics=True)\n",
    "val_dataset = MindDataset(val_samples, include_topics=True)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BACTH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn= lambda x: mind_collate_fn(x, include_topics=True)\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BACTH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn= lambda x: mind_collate_fn(x, include_topics=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NRMS(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_embed_word = 128,\n",
    "    d_embed_news = 256,\n",
    "    n_heads_news = 8,\n",
    "    n_heads_user = 8,\n",
    "    d_mlp_news = 512,\n",
    "    d_mlp_user = 512,\n",
    "    news_layers = 1,\n",
    "    user_layers = 1,\n",
    "    dropout = 0.1,\n",
    "    pad_max_len = MAX_TITLE_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5,092,480'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{sum(p.numel() for p in model.parameters()): ,}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 1e-4,\n",
    "    device: str = \"cuda\",\n",
    "    log_interval: int = 100,\n",
    "    checkpoint_interval: int = 10000,\n",
    "    project_name: str = \"NRMS\",\n",
    "    save_path: str = \"./checkpoints_topic/\",\n",
    "    smoothing_alpha: float = 0.8,  # EWMA smoothing factor\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains `model` using train_dataloader, evaluates on val_dataloader each epoch,\n",
    "    logs EWMA-smoothed loss + MRR + learning rate to W&B, and finally saves model parameters.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"loss_fn\": \"CrossEntropyLoss\",\n",
    "            \"metric\": \"MRR\",\n",
    "            \"smoothing_alpha\": smoothing_alpha,\n",
    "            \"target_labels\": \"Topics\",\n",
    "            \n",
    "            \"model_def\": \t\"\"\"\n",
    "\t\t\t\t\t\t\tmodel = NRMS(\n",
    "\t\t\t\t\t\t\t\tvocab_size=tokenizer.vocab_size,\n",
    "\t\t\t\t\t\t\t\td_embed_word = 128,\n",
    "\t\t\t\t\t\t\t\td_embed_news = 256,\n",
    "\t\t\t\t\t\t\t\tn_heads_news = 8,\n",
    "\t\t\t\t\t\t\t\tn_heads_user = 8,\n",
    "\t\t\t\t\t\t\t\td_mlp_news = 512,\n",
    "\t\t\t\t\t\t\t\td_mlp_user = 512,\n",
    "\t\t\t\t\t\t\t\tnews_layers = 1,\n",
    "\t\t\t\t\t\t\t\tuser_layers = 1,\n",
    "\t\t\t\t\t\t\t\tdropout = 0.1,\n",
    "\t\t\t\t\t\t\t\tpad_max_len = MAX_TITLE_LEN,\n",
    "\t\t\t\t\t\t\t)\"\"\"                                  \n",
    "        },\n",
    "    )\n",
    "    # Only log weight histograms (no gradients) to cut down on storage\n",
    "    wandb.watch(model, log=\"parameters\", log_freq=500)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    # === Scheduler Setup ===\n",
    "    # total_steps = epochs * number_of_batches_per_epoch\n",
    "    total_steps = epochs * len(train_dataloader)\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    # =======================\n",
    "\n",
    "    step = 0  # global batch index\n",
    "\n",
    "    # Initialize EWMA trackers\n",
    "    ewma_loss = None\n",
    "    ewma_mrr = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ##### Training Phase #####\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_mrr = 0.0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for clicked_ids,clicked_mask, _, _, cand_topic_ids, cand_topic_mask, labels in tqdm(\n",
    "            train_dataloader, desc=f\"Epoch {epoch} [Train]\"\n",
    "        ):\n",
    "            clicked_ids = clicked_ids.to(device)\n",
    "            clicked_mask = clicked_mask.to(device)\n",
    "            cand_ids = cand_topic_ids.to(device)\n",
    "            cand_mask = cand_topic_mask.to(device)\n",
    "            labels = labels.to(device)  # (B,)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass → logits of shape (B, K)\n",
    "            scores: torch.Tensor = model(\n",
    "                clicked_ids, ~clicked_mask, cand_ids, cand_mask\n",
    "            )  # (B, K)\n",
    "            loss = criterion(scores, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # <-- step the scheduler immediately after optimizer\n",
    "\n",
    "            # Compute batch MRR\n",
    "            with torch.no_grad():\n",
    "                batch_size = labels.size(0)\n",
    "                batch_mrr = 0.0\n",
    "                for i in range(batch_size):\n",
    "                    true_idx = labels[i].item()\n",
    "                    sorted_indices = scores[i].argsort(descending=True)\n",
    "                    rank = (sorted_indices == true_idx).nonzero(as_tuple=False).item() + 1\n",
    "                    batch_mrr += 1.0 / rank\n",
    "                batch_mrr /= batch_size\n",
    "\n",
    "            # Update EWMA for loss\n",
    "            if ewma_loss is None:\n",
    "                ewma_loss = loss.item()\n",
    "            else:\n",
    "                ewma_loss = smoothing_alpha * ewma_loss + (1.0 - smoothing_alpha) * loss.item()\n",
    "\n",
    "            # Update EWMA for MRR\n",
    "            if ewma_mrr is None:\n",
    "                ewma_mrr = batch_mrr\n",
    "            else:\n",
    "                ewma_mrr = smoothing_alpha * ewma_mrr + (1.0 - smoothing_alpha) * batch_mrr\n",
    "\n",
    "            # Accumulate totals (for printing at end of epoch)\n",
    "            total_train_loss += loss.item() * batch_size\n",
    "            total_train_mrr += batch_mrr * batch_size\n",
    "            total_train_samples += batch_size\n",
    "\n",
    "            # Log smoothed scalars (loss, MRR, lr) to W&B every log_interval steps\n",
    "            if step % log_interval == 0:\n",
    "                current_lr = scheduler.get_last_lr()[0]\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/loss_EWMA\": ewma_loss,\n",
    "                        \"train/MRR_EWMA\": ewma_mrr,\n",
    "                        \"train/lr\": current_lr,\n",
    "                        \"epoch\": epoch,\n",
    "                    },\n",
    "                    step=step,\n",
    "                )\n",
    "\n",
    "            # Save checkpoint periodically\n",
    "            if step % checkpoint_interval == 0 and step > 0:\n",
    "                print(f\"Saving checkpoint at step {step}...\")\n",
    "                checkpoint_path = save_path + f\"checkpoint_epoch{epoch}_step{step}.pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # End of epoch: compute raw averages\n",
    "        avg_train_loss = total_train_loss / total_train_samples\n",
    "        avg_train_mrr = total_train_mrr / total_train_samples\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train MRR: {avg_train_mrr:.4f}\"\n",
    "        )\n",
    "\n",
    "        ##### Validation Phase #####\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_val_mrr = 0.0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for clicked_ids,clicked_mask, _, _, cand_topic_ids, cand_topic_mask, labels in tqdm(\n",
    "                val_dataloader, desc=f\"Epoch {epoch} [Val]\"\n",
    "            ):\n",
    "                clicked_ids = clicked_ids.to(device)\n",
    "                clicked_mask = clicked_mask.to(device)\n",
    "                cand_ids = cand_topic_ids.to(device)\n",
    "                cand_mask = cand_topic_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                scores = model(clicked_ids, ~clicked_mask, cand_ids, cand_mask)\n",
    "                loss = criterion(scores, labels)\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                batch_mrr = 0.0\n",
    "                for i in range(batch_size):\n",
    "                    true_idx = labels[i].item()\n",
    "                    sorted_indices = scores[i].argsort(descending=True)\n",
    "                    rank = (sorted_indices == true_idx).nonzero(as_tuple=False).item() + 1\n",
    "                    batch_mrr += 1.0 / rank\n",
    "                batch_mrr /= batch_size\n",
    "\n",
    "                total_val_loss += loss.item() * batch_size\n",
    "                total_val_mrr += batch_mrr * batch_size\n",
    "                total_val_samples += batch_size\n",
    "\n",
    "        avg_val_loss = total_val_loss / total_val_samples\n",
    "        avg_val_mrr = total_val_mrr / total_val_samples\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Val Loss:   {avg_val_loss:.4f}, Val MRR:   {avg_val_mrr:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Log validation metrics to W&B once per epoch\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"val/loss\": avg_val_loss,\n",
    "                \"val/MRR\": avg_val_mrr,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=step,\n",
    "        )\n",
    "\n",
    "        # Save model checkpoint at end of epoch\n",
    "        print(f\"Saving model parameters for epoch {epoch}...\")\n",
    "        checkpoint_path = save_path + f\"checkpoint_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "    # Finish the W&B run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielvolkov\u001b[0m (\u001b[33mthe_magnivim\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/danik/Documents/hermes/model/NRMS/wandb/run-20250628_152423-1a3x4thx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/the_magnivim/NRMS/runs/1a3x4thx' target=\"_blank\">desert-bush-50</a></strong> to <a href='https://wandb.ai/the_magnivim/NRMS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/the_magnivim/NRMS' target=\"_blank\">https://wandb.ai/the_magnivim/NRMS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/the_magnivim/NRMS/runs/1a3x4thx' target=\"_blank\">https://wandb.ai/the_magnivim/NRMS/runs/1a3x4thx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   4%|▍         | 1002/26161 [02:19<58:01,  7.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 1000...\n",
      "Checkpoint saved to ./checkpoints_topic/checkpoint_epoch1_step1000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   8%|▊         | 2002/26161 [04:33<52:16,  7.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 2000...\n",
      "Checkpoint saved to ./checkpoints_topic/checkpoint_epoch1_step2000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  11%|█▏        | 3002/26161 [06:49<51:03,  7.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 3000...\n",
      "Checkpoint saved to ./checkpoints_topic/checkpoint_epoch1_step3000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  13%|█▎        | 3517/26161 [08:04<51:59,  7.26it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNRMS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./checkpoints_topic/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_dataloader, val_dataloader, epochs, lr, device, log_interval, checkpoint_interval, project_name, save_path, smoothing_alpha)\u001b[39m\n\u001b[32m    109\u001b[39m batch_mrr = \u001b[32m0.0\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     true_idx = \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     sorted_indices = scores[i].argsort(descending=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    113\u001b[39m     rank = (sorted_indices == true_idx).nonzero(as_tuple=\u001b[38;5;28;01mFalse\u001b[39;00m).item() + \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    epochs=2,\n",
    "    lr=1e-4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_interval=100,\n",
    "    checkpoint_interval=1000,\n",
    "    project_name=\"NRMS\",\n",
    "    save_path=\"./checkpoints_topic/\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
