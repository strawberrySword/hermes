{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nrms import NRMS\n",
    "from typing import List, Dict\n",
    "from torch.optim.adamw import AdamW\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, get_linear_schedule_with_warmup\n",
    "from data import load_and_tokenize_news, load_behaviors, MindDataset, mind_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = './data/MIND_'\n",
    "\n",
    "\n",
    "MAX_TITLE_LEN = 100   # each headline → exactly MAX_TITLE_LEN tokens (truncated/padded)\n",
    "MAX_HISTORY  = 50     # each user’s clicked history → exactly MAX_HISTORY articles\n",
    "BACTH_SIZE = 6\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "train_news_dict = load_and_tokenize_news(BASE_DATA_DIR+'train/news.tsv', tokenizer, MAX_TITLE_LEN)\n",
    "train_samples   = load_behaviors(BASE_DATA_DIR+'train/behaviors.tsv', train_news_dict, MAX_HISTORY)\n",
    "\n",
    "val_news_dict = load_and_tokenize_news(BASE_DATA_DIR+'val/news.tsv', tokenizer, MAX_TITLE_LEN)\n",
    "val_samples   = load_behaviors(BASE_DATA_DIR+'val/behaviors.tsv', val_news_dict, MAX_HISTORY)\n",
    "\n",
    "\n",
    "train_dataset = MindDataset(train_samples)\n",
    "val_dataset = MindDataset(val_samples)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BACTH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=mind_collate_fn\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BACTH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=mind_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NRMS(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_embed_word = 128,\n",
    "    d_embed_news = 256,\n",
    "    n_heads_news = 8,\n",
    "    n_heads_user = 8,\n",
    "    d_mlp_news = 512,\n",
    "    d_mlp_user = 512,\n",
    "    news_layers = 1,\n",
    "    user_layers = 1,\n",
    "    dropout = 0.1,\n",
    "    pad_max_len = MAX_TITLE_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5,092,480'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{sum(p.numel() for p in model.parameters()): ,}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 1e-4,\n",
    "    device: str = \"cuda\",\n",
    "    log_interval: int = 100,\n",
    "    checkpoint_interval: int = 10000,\n",
    "    project_name: str = \"NRMS\",\n",
    "    save_path: str = \"./checkpoints/\",\n",
    "    smoothing_alpha: float = 0.6,  # EWMA smoothing factor\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains `model` using train_dataloader, evaluates on val_dataloader each epoch,\n",
    "    logs EWMA-smoothed loss + MRR + learning rate to W&B, and finally saves model parameters.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"loss_fn\": \"CrossEntropyLoss\",\n",
    "            \"metric\": \"MRR\",\n",
    "            \"smoothing_alpha\": smoothing_alpha,\n",
    "        },\n",
    "    )\n",
    "    # Only log weight histograms (no gradients) to cut down on storage\n",
    "    wandb.watch(model, log=\"parameters\", log_freq=500)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    # === Scheduler Setup ===\n",
    "    # total_steps = epochs * number_of_batches_per_epoch\n",
    "    total_steps = epochs * len(train_dataloader)\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    # =======================\n",
    "\n",
    "    step = 0  # global batch index\n",
    "\n",
    "    # Initialize EWMA trackers\n",
    "    ewma_loss = None\n",
    "    ewma_mrr = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ##### Training Phase #####\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_mrr = 0.0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for clicked_ids, clicked_mask, cand_ids, cand_mask, labels in tqdm(\n",
    "            train_dataloader, desc=f\"Epoch {epoch} [Train]\"\n",
    "        ):\n",
    "            clicked_ids = clicked_ids.to(device)\n",
    "            clicked_mask = clicked_mask.to(device)\n",
    "            cand_ids = cand_ids.to(device)\n",
    "            cand_mask = cand_mask.to(device)\n",
    "            labels = labels.to(device)  # (B,)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass → logits of shape (B, K)\n",
    "            scores: torch.Tensor = model(\n",
    "                clicked_ids, ~clicked_mask, cand_ids, cand_mask\n",
    "            )  # (B, K)\n",
    "            loss = criterion(scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # <-- step the scheduler immediately after optimizer\n",
    "\n",
    "            # Compute batch MRR\n",
    "            with torch.no_grad():\n",
    "                batch_size = labels.size(0)\n",
    "                batch_mrr = 0.0\n",
    "                for i in range(batch_size):\n",
    "                    true_idx = labels[i].item()\n",
    "                    sorted_indices = scores[i].argsort(descending=True)\n",
    "                    rank = (sorted_indices == true_idx).nonzero(as_tuple=False).item() + 1\n",
    "                    batch_mrr += 1.0 / rank\n",
    "                batch_mrr /= batch_size\n",
    "\n",
    "            # Update EWMA for loss\n",
    "            if ewma_loss is None:\n",
    "                ewma_loss = loss.item()\n",
    "            else:\n",
    "                ewma_loss = smoothing_alpha * ewma_loss + (1.0 - smoothing_alpha) * loss.item()\n",
    "\n",
    "            # Update EWMA for MRR\n",
    "            if ewma_mrr is None:\n",
    "                ewma_mrr = batch_mrr\n",
    "            else:\n",
    "                ewma_mrr = smoothing_alpha * ewma_mrr + (1.0 - smoothing_alpha) * batch_mrr\n",
    "\n",
    "            # Accumulate totals (for printing at end of epoch)\n",
    "            total_train_loss += loss.item() * batch_size\n",
    "            total_train_mrr += batch_mrr * batch_size\n",
    "            total_train_samples += batch_size\n",
    "\n",
    "            # Log smoothed scalars (loss, MRR, lr) to W&B every log_interval steps\n",
    "            if step % log_interval == 0:\n",
    "                current_lr = scheduler.get_last_lr()[0]\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/loss_EWMA\": ewma_loss,\n",
    "                        \"train/MRR_EWMA\": ewma_mrr,\n",
    "                        \"train/lr\": current_lr,\n",
    "                        \"epoch\": epoch,\n",
    "                    },\n",
    "                    step=step,\n",
    "                )\n",
    "\n",
    "            # Save checkpoint periodically\n",
    "            if step % checkpoint_interval == 0 and step > 0:\n",
    "                print(f\"Saving checkpoint at step {step}...\")\n",
    "                checkpoint_path = save_path + f\"checkpoint_epoch{epoch}_step{step}.pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # End of epoch: compute raw averages\n",
    "        avg_train_loss = total_train_loss / total_train_samples\n",
    "        avg_train_mrr = total_train_mrr / total_train_samples\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train MRR: {avg_train_mrr:.4f}\"\n",
    "        )\n",
    "\n",
    "        ##### Validation Phase #####\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_val_mrr = 0.0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for clicked_ids, clicked_mask, cand_ids, cand_mask, labels in tqdm(\n",
    "                val_dataloader, desc=f\"Epoch {epoch} [Val]\"\n",
    "            ):\n",
    "                clicked_ids = clicked_ids.to(device)\n",
    "                clicked_mask = clicked_mask.to(device)\n",
    "                cand_ids = cand_ids.to(device)\n",
    "                cand_mask = cand_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                scores = model(clicked_ids, ~clicked_mask, cand_ids, cand_mask)\n",
    "                loss = criterion(scores, labels)\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                batch_mrr = 0.0\n",
    "                for i in range(batch_size):\n",
    "                    true_idx = labels[i].item()\n",
    "                    sorted_indices = scores[i].argsort(descending=True)\n",
    "                    rank = (sorted_indices == true_idx).nonzero(as_tuple=False).item() + 1\n",
    "                    batch_mrr += 1.0 / rank\n",
    "                batch_mrr /= batch_size\n",
    "\n",
    "                total_val_loss += loss.item() * batch_size\n",
    "                total_val_mrr += batch_mrr * batch_size\n",
    "                total_val_samples += batch_size\n",
    "\n",
    "        avg_val_loss = total_val_loss / total_val_samples\n",
    "        avg_val_mrr = total_val_mrr / total_val_samples\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Val Loss:   {avg_val_loss:.4f}, Val MRR:   {avg_val_mrr:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Log validation metrics to W&B once per epoch\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"val/loss\": avg_val_loss,\n",
    "                \"val/MRR\": avg_val_mrr,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=step,\n",
    "        )\n",
    "\n",
    "        # Save model checkpoint at end of epoch\n",
    "        print(f\"Saving model parameters for epoch {epoch}...\")\n",
    "        checkpoint_path = save_path + f\"checkpoint_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "    # Finish the W&B run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: danielvolkov (the_magnivim) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c64d8ff1c3142c5b27a3f624edff973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Danik\\Documents\\TAU\\25B\\Workshop on RecSys\\hermes\\model\\NRMS\\wandb\\run-20250606_025212-okxl6zuz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/the_magnivim/NRMS/runs/okxl6zuz' target=\"_blank\">eager-dust-14</a></strong> to <a href='https://wandb.ai/the_magnivim/NRMS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/the_magnivim/NRMS' target=\"_blank\">https://wandb.ai/the_magnivim/NRMS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/the_magnivim/NRMS/runs/okxl6zuz' target=\"_blank\">https://wandb.ai/the_magnivim/NRMS/runs/okxl6zuz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/26161 [00:00<?, ?it/s]c:\\Users\\Danik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Epoch 1 [Train]:   4%|▍         | 1000/26161 [02:28<1:06:06,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 1000...\n",
      "Checkpoint saved to ./checkpoints/checkpoint_epoch1_step1000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   5%|▌         | 1423/26161 [03:35<1:23:34,  4.93it/s]"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    epochs=2,\n",
    "    lr=1e-4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_interval=100,\n",
    "    checkpoint_interval=1000,\n",
    "    project_name=\"NRMS\",\n",
    "    save_path=\"./checkpoints/\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
