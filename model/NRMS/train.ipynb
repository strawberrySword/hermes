{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nrms import NRMS\n",
    "from typing import List, Dict\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch_optimizer import AdamP\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from data import load_and_tokenize_news, load_behaviors, MindDataset, mind_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = './data/MIND_'\n",
    "\n",
    "\n",
    "MAX_TITLE_LEN = 100   # each headline → exactly MAX_TITLE_LEN tokens (truncated/padded)\n",
    "MAX_HISTORY  = 50     # each user’s clicked history → exactly MAX_HISTORY articles\n",
    "BACTH_SIZE = 6\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "train_news_dict = load_and_tokenize_news(BASE_DATA_DIR+'train/news.tsv', tokenizer, MAX_TITLE_LEN)\n",
    "train_samples   = load_behaviors(BASE_DATA_DIR+'train/behaviors.tsv', train_news_dict, MAX_HISTORY)\n",
    "\n",
    "val_news_dict = load_and_tokenize_news(BASE_DATA_DIR+'val/news.tsv', tokenizer, MAX_TITLE_LEN)\n",
    "val_samples   = load_behaviors(BASE_DATA_DIR+'val/behaviors.tsv', val_news_dict, MAX_HISTORY)\n",
    "\n",
    "\n",
    "train_dataset = MindDataset(train_samples)\n",
    "val_dataset = MindDataset(val_samples)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BACTH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=mind_collate_fn\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BACTH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=mind_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NRMS(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_embed_word = 128,\n",
    "    d_embed_news = 256,\n",
    "    n_heads_news = 8,\n",
    "    n_heads_user = 8,\n",
    "    d_mlp_news = 512,\n",
    "    d_mlp_user = 512,\n",
    "    news_layers = 1,\n",
    "    user_layers = 1,\n",
    "    dropout = 0.1,\n",
    "    pad_max_len = MAX_TITLE_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5,092,480'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{sum(p.numel() for p in model.parameters()): ,}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 1e-4,\n",
    "    device: str = \"cuda\",\n",
    "    log_interval: int = 100,\n",
    "    checkpoint_interval: int = 10000,\n",
    "    project_name: str = \"NRMS\",\n",
    "    save_path: str = \"./checkpoints/\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Trains `model` using train_dataloader, evaluates on val_dataloader each epoch,\n",
    "    logs metrics to W&B, and finally saves model parameters to `save_path`.\n",
    "\n",
    "    Args:\n",
    "        model: a PyTorch nn.Module that returns logits of shape (B, K) given:\n",
    "               (clicked_ids, clicked_mask, cand_ids, cand_mask)\n",
    "        train_dataloader: torch.utils.data.DataLoader for training\n",
    "        val_dataloader: torch.utils.data.DataLoader for validation\n",
    "        epochs: number of epochs to train\n",
    "        lr: learning rate for Adam\n",
    "        device: \"cuda\" or \"cpu\"\n",
    "        project_name: W&B project name\n",
    "        save_path: where to save the final model.state_dict()\n",
    "    \"\"\"\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"loss_fn\": \"CrossEntropyLoss\",\n",
    "        },\n",
    "    )\n",
    "    wandb.watch(model, log=\"parameters\", log_freq=500)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=lr,\n",
    "                      betas=(0.9, 0.999),\n",
    "                      eps=1e-8,\n",
    "                      weight_decay=1e-4\n",
    "                )\n",
    "\n",
    "    step = 0 # increases each batch\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ##### Training Phase #####\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for clicked_ids, clicked_mask, cand_ids, cand_mask, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch} [Train]\"):\n",
    "            clicked_ids = clicked_ids.to(device)\n",
    "            clicked_mask = clicked_mask.to(device)\n",
    "            cand_ids = cand_ids.to(device)\n",
    "            cand_mask = cand_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(clicked_ids, ~clicked_mask, cand_ids, cand_mask)\n",
    "\n",
    "            # Compute training loss\n",
    "            loss = criterion(scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate stats\n",
    "            total_train_loss += loss.item() * labels.size(0)\n",
    "            preds = scores.argmax(dim=1)\n",
    "            total_train_correct += (preds == labels).sum().item()\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "            if step % log_interval == 0:\n",
    "                # Log to console\n",
    "                # print(\n",
    "                #     f\"Step {step} | \"\n",
    "                #     f\"Train Loss: {loss.item():.4f}, \"\n",
    "                #     f\"Train Acc: {(preds == labels).float().mean().item():.4f}\"\n",
    "                # )\n",
    "                # Log to W&B\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/loss\": loss.item(),\n",
    "                        \"train/accuracy\": (preds == labels).float().mean().item(),\n",
    "                        \"step\": step,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if step % checkpoint_interval == 0:\n",
    "                print(f'Saving checkpoint at step {step}...')\n",
    "                checkpoint_path = save_path + f\"checkpoint_epoch{epoch}_step{step}.pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "            step += 1\n",
    "\n",
    "\n",
    "        avg_train_loss = total_train_loss / total_train_samples\n",
    "        train_accuracy = total_train_correct / total_train_samples\n",
    "\n",
    "        ##### Validation Phase #####\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for clicked_ids, clicked_mask, cand_ids, cand_mask, labels in tqdm(val_dataloader, desc=f\"Epoch {epoch} [Val]\"):\n",
    "                clicked_ids = clicked_ids.to(device)\n",
    "                clicked_mask = clicked_mask.to(device)\n",
    "                cand_ids = cand_ids.to(device)\n",
    "                cand_mask = cand_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                scores = model(clicked_ids, ~clicked_mask, cand_ids, cand_mask)\n",
    "\n",
    "                # Compute validation loss\n",
    "                loss = criterion(scores, labels)\n",
    "                total_val_loss += loss.item() * labels.size(0)\n",
    "\n",
    "                preds = scores.argmax(dim=1)\n",
    "                total_val_correct += (preds == labels).sum().item()\n",
    "                total_val_samples += labels.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / total_val_samples\n",
    "        val_accuracy = total_val_correct / total_val_samples\n",
    "\n",
    "        # Log to console\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Log to W&B\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"val/loss\": avg_val_loss,\n",
    "                \"val/accuracy\": val_accuracy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save model checkpoint after epoch\n",
    "        print(f\"Saving model parameters for epoch {epoch}...\")\n",
    "        checkpoint_path = save_path + f\"checkpoint_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "    # Finish the W&B run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 1e-4,\n",
    "    device: str = \"cuda\",\n",
    "    log_interval: int = 100,\n",
    "    checkpoint_interval: int = 10000,\n",
    "    project_name: str = \"NRMS\",\n",
    "    save_path: str = \"./checkpoints/\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains `model` using train_dataloader, evaluates on val_dataloader each epoch,\n",
    "    logs loss + MRR to W&B (with step as the x-axis), and finally saves model parameters.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"loss_fn\": \"CrossEntropyLoss\",\n",
    "            \"metric\": \"MRR\",\n",
    "        },\n",
    "    )\n",
    "    # Only log weight histograms (no gradients) to cut down on storage\n",
    "    wandb.watch(model, log=\"parameters\", log_freq=500)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=lr,\n",
    "                      betas=(0.9, 0.999),\n",
    "                      eps=1e-8,\n",
    "                      weight_decay=1e-4\n",
    "                )\n",
    "\n",
    "    step = 0  # global batch index\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ##### Training Phase #####\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_mrr = 0.0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for clicked_ids, clicked_mask, cand_ids, cand_mask, labels in tqdm(\n",
    "            train_dataloader, desc=f\"Epoch {epoch} [Train]\"\n",
    "        ):\n",
    "            clicked_ids = clicked_ids.to(device)\n",
    "            clicked_mask = clicked_mask.to(device)\n",
    "            cand_ids = cand_ids.to(device)\n",
    "            cand_mask = cand_mask.to(device)\n",
    "            labels = labels.to(device)  # (B,)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass → logits of shape (B, K)\n",
    "            scores: torch.Tensor = model(\n",
    "                clicked_ids, ~clicked_mask, cand_ids, cand_mask\n",
    "            )  # (B, K)\n",
    "            loss = criterion(scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute batch MRR\n",
    "            with torch.no_grad():\n",
    "                batch_size = labels.size(0)\n",
    "                batch_mrr = 0.0\n",
    "                for i in range(batch_size):\n",
    "                    true_idx = labels[i].item()\n",
    "                    sorted_indices = scores[i].argsort(descending=True)\n",
    "                    rank = (sorted_indices == true_idx).nonzero(as_tuple=False).item() + 1\n",
    "                    batch_mrr += 1.0 / rank\n",
    "                batch_mrr /= batch_size\n",
    "\n",
    "            # Accumulate totals (for printing at end of epoch)\n",
    "            total_train_loss += loss.item() * batch_size\n",
    "            total_train_mrr += batch_mrr * batch_size\n",
    "            total_train_samples += batch_size\n",
    "\n",
    "            # Log scalars to W&B every log_interval steps\n",
    "            if step % log_interval == 0:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/loss\": loss.item(),\n",
    "                        \"train/MRR\": batch_mrr,\n",
    "                        \"epoch\": epoch,\n",
    "                    },\n",
    "                    step=step,\n",
    "                )\n",
    "\n",
    "            # Save checkpoint periodically\n",
    "            if step % checkpoint_interval == 0 and step > 0:\n",
    "                print(f\"Saving checkpoint at step {step}...\")\n",
    "                checkpoint_path = save_path + f\"checkpoint_epoch{epoch}_step{step}.pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # End of epoch: compute averages\n",
    "        avg_train_loss = total_train_loss / total_train_samples\n",
    "        avg_train_mrr = total_train_mrr / total_train_samples\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train MRR: {avg_train_mrr:.4f}\"\n",
    "        )\n",
    "\n",
    "        ##### Validation Phase #####\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_val_mrr = 0.0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for clicked_ids, clicked_mask, cand_ids, cand_mask, labels in tqdm(\n",
    "                val_dataloader, desc=f\"Epoch {epoch} [Val]\"\n",
    "            ):\n",
    "                clicked_ids = clicked_ids.to(device)\n",
    "                clicked_mask = clicked_mask.to(device)\n",
    "                cand_ids = cand_ids.to(device)\n",
    "                cand_mask = cand_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                scores = model(clicked_ids, ~clicked_mask, cand_ids, cand_mask)\n",
    "                loss = criterion(scores, labels)\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                batch_mrr = 0.0\n",
    "                for i in range(batch_size):\n",
    "                    true_idx = labels[i].item()\n",
    "                    sorted_indices = scores[i].argsort(descending=True)\n",
    "                    rank = (sorted_indices == true_idx).nonzero(as_tuple=False).item() + 1\n",
    "                    batch_mrr += 1.0 / rank\n",
    "                batch_mrr /= batch_size\n",
    "\n",
    "                total_val_loss += loss.item() * batch_size\n",
    "                total_val_mrr += batch_mrr * batch_size\n",
    "                total_val_samples += batch_size\n",
    "\n",
    "        avg_val_loss = total_val_loss / total_val_samples\n",
    "        avg_val_mrr = total_val_mrr / total_val_samples\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Val Loss:   {avg_val_loss:.4f}, Val MRR:   {avg_val_mrr:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Log validation metrics to W&B (use current step so they share the same x-axis if desired)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"val/loss\": avg_val_loss,\n",
    "                \"val/MRR\": avg_val_mrr,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=step,\n",
    "        )\n",
    "\n",
    "        # Save model checkpoint at end of epoch\n",
    "        print(f\"Saving model parameters for epoch {epoch}...\")\n",
    "        checkpoint_path = save_path + f\"checkpoint_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "    # Finish the W&B run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: danielvolkov (the_magnivim) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Danik\\Documents\\TAU\\25B\\Workshop on RecSys\\hermes\\model\\NRMS\\wandb\\run-20250606_015019-mh7de8jb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/the_magnivim/NRMS/runs/mh7de8jb' target=\"_blank\">distinctive-glitter-9</a></strong> to <a href='https://wandb.ai/the_magnivim/NRMS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/the_magnivim/NRMS' target=\"_blank\">https://wandb.ai/the_magnivim/NRMS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/the_magnivim/NRMS/runs/mh7de8jb' target=\"_blank\">https://wandb.ai/the_magnivim/NRMS/runs/mh7de8jb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/26161 [00:00<?, ?it/s]c:\\Users\\Danik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Epoch 1 [Train]:   4%|▍         | 1000/26161 [02:27<1:01:43,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 1000...\n",
      "Checkpoint saved to ./checkpoints/checkpoint_epoch1_step1000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   8%|▊         | 2000/26161 [04:57<1:10:47,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 2000...\n",
      "Checkpoint saved to ./checkpoints/checkpoint_epoch1_step2000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  11%|█▏        | 3000/26161 [07:22<59:06,  6.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 3000...\n",
      "Checkpoint saved to ./checkpoints/checkpoint_epoch1_step3000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  15%|█▌        | 4000/26161 [09:48<52:09,  7.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 4000...\n",
      "Checkpoint saved to ./checkpoints/checkpoint_epoch1_step4000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  19%|█▊        | 4888/26161 [12:03<52:28,  6.76it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNRMS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoints/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 69\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataloader, epochs, lr, device, log_interval, checkpoint_interval, project_name, save_path)\u001b[0m\n\u001b[0;32m     67\u001b[0m batch_mrr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m---> 69\u001b[0m     true_idx \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     sorted_indices \u001b[38;5;241m=\u001b[39m scores[i]\u001b[38;5;241m.\u001b[39margsort(descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     71\u001b[0m     rank \u001b[38;5;241m=\u001b[39m (sorted_indices \u001b[38;5;241m==\u001b[39m true_idx)\u001b[38;5;241m.\u001b[39mnonzero(as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    epochs=2,\n",
    "    lr=1e-4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_interval=100,\n",
    "    checkpoint_interval=1000,\n",
    "    project_name=\"NRMS\",\n",
    "    save_path=\"./checkpoints/\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
